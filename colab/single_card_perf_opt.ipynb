{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNBIAgyxPMY0/O1jSRNhk/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmars/n00bGPT/blob/main/colab/single_card_perf_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylQyRcDFkYZq",
        "outputId": "3ede3bd7-6c6b-4387-d684-9d60866213eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.10/dist-packages (7.352.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate nvidia-ml-py3 accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "import torch\n",
        "import math\n",
        "import transformers\n",
        "from dataclasses import dataclass\n",
        "from accelerate import Accelerator\n"
      ],
      "metadata": {
        "id": "s1RpJ46Pkbzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gpu_util(s=''):\n",
        "    nvmlInit()\n",
        "    gb = 1024**3\n",
        "    t = torch.cuda.get_device_properties(0).total_memory\n",
        "    r = torch.cuda.memory_reserved(0)\n",
        "    a = torch.cuda.memory_allocated(0)\n",
        "    f = t-a  # free inside reserved\n",
        "    p = torch.cuda.max_memory_allocated()\n",
        "    print(f'{s:<25}: GPU memory (Torch) total: {t/gb:.1f}, reserved: {r/gb:.1f}, allocated: {a/gb:.1f}, free: {f/gb:.1f}, peak: {p/gb:.1f}')\n",
        "\n",
        "gpu_util()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f258lUYkgKQ",
        "outputId": "b965a4bf-4037-4757-806f-23f1310c2904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         : GPU memory (Torch) total: 15.8, reserved: 0.0, allocated: 0.0, free: 15.8, peak: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File: dadadaada1.py\n",
        "\n",
        "def softmax(x):\n",
        "  # Note, without normalization it suffers from instability\n",
        "  # in attention computation, causing either NaN or wrong result\n",
        "  x = x - torch.max(x,axis=-1,keepdim=True)[0]\n",
        "  exp = torch.exp(x)\n",
        "  sum = torch.sum(exp, dim=-1, keepdim=True)\n",
        "  return exp / sum\n",
        "\n",
        "def scaled_dot_product_attention(\n",
        "    q: torch.Tensor, # [N,...,L,E]\n",
        "    k: torch.Tensor, # [N,...,L,E]\n",
        "    v: torch.Tensor, # [N,...,L,E]\n",
        ") -> torch.Tensor:\n",
        "    t = torch.matmul(q, k.transpose(-1,-2))\n",
        "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)\n",
        "    t = t / torch.sqrt(dk)\n",
        "    t=softmax(t)\n",
        "    return torch.matmul(t, v)\n",
        "\n",
        "class MultiheadAttentionNoVectorization(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, n_head):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_head = n_head\n",
        "    self.head_dim = emb_dim // n_head\n",
        "    assert self.head_dim * n_head == emb_dim\n",
        "    self.qw = torch.randn(emb_dim, n_head, self.head_dim) # [emb_dim, n_head, head_dim]\n",
        "    self.kw = torch.randn(emb_dim, n_head, self.head_dim) # [emb_dim, n_head, head_dim]\n",
        "    self.vw = torch.randn(emb_dim, n_head, self.head_dim) # [emb_dim, n_head, head_dim]\n",
        "    self.w = torch.randn(emb_dim, emb_dim)\n",
        "\n",
        "  def forward(self, q,k,v):\n",
        "    # q,k,v: [n_batch, seq_len, emb_dim]\n",
        "    heads = []\n",
        "    for i in range(self.n_head):\n",
        "      qi = torch.matmul(q, self.qw[:,i,:]) # [n_batch, seq_len, head_dim]\n",
        "      ki = torch.matmul(k, self.kw[:,i,:]) # [n_batch, seq_len, head_dim]\n",
        "      vi = torch.matmul(v, self.vw[:,i,:]) # [n_batch, seq_len, head_dim]\n",
        "      head = torch.nn.functional.scaled_dot_product_attention(qi, ki, vi) # [n_batch, seq_len, head_dim]\n",
        "      heads.append(head)\n",
        "    concated = torch.concat(heads, dim=-1) # [n_batch, seq_len, emb_dim]\n",
        "    out = torch.matmul(concated, self.w) # [n_batch, seq_len, emb_dim]\n",
        "    return out\n",
        "\n",
        "class MultiheadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, n_head):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_head = n_head\n",
        "    self.head_dim = emb_dim // n_head\n",
        "    assert self.head_dim * self.n_head == self.emb_dim\n",
        "    self.bias = True\n",
        "\n",
        "    self.attn = torch.nn.Linear(self.emb_dim, 3 * self.emb_dim, bias = self.bias)\n",
        "    self.proj = torch.nn.Linear(self.emb_dim, self.emb_dim, bias=self.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: [n_batch, seq_len, emb_dim], assuming q=k=v=x in MultiheadAttentionSimple implementation\n",
        "    n_batch, seq_len, emb_dim = x.shape\n",
        "    # Vectorized/concated form of QW_q_i, KW_k_i, and VW_v_i\n",
        "    attn = self.attn(x) # [n_batch, seq_len, 3*emb_dim]\n",
        "    q,k,v = attn.split(self.emb_dim, dim=-1)\n",
        "    # Reshape to per-head form\n",
        "    q = q.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    k = k.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    v = v.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    # Compute dot product attention, which input is [N,..., seq_len, emb_dim]\n",
        "    y = torch.nn.functional.scaled_dot_product_attention(q,k,v, is_causal=True) # [n_batch, n_head, seq_len, head_dim]\n",
        "    y = y.transpose(1,2).contiguous().view(n_batch, seq_len, emb_dim) # [n_batch, seq_len, emb_dim]\n",
        "    y = self.proj(y)\n",
        "    return y\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, bias, dropout):\n",
        "    super().__init__()\n",
        "    self.context_proj_1 = torch.nn.Linear(emb_dim, 4 *emb_dim, bias=bias)\n",
        "    # Note, torch.nn.GELU() and hf.gelu_new behaves differently, though\n",
        "    # it doesn't seem so from wiki doc\n",
        "    self.gelu = transformers.activations.ACT2FN['gelu_new']\n",
        "    self.context_proj_2 = torch.nn.Linear(4*emb_dim, emb_dim, bias=bias)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.context_proj_1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.context_proj_2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "class Layer(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, bias, dropout):\n",
        "    super().__init__()\n",
        "    self.ln_1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.attn = MultiheadAttention(emb_dim, n_head)\n",
        "    self.ln_2 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.feed_fwd = FeedForward(emb_dim, bias, dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.ln_1(x)\n",
        "    x = self.attn(x)\n",
        "    x = x + residual\n",
        "    residual = x\n",
        "    x = self.ln_2(x)\n",
        "    x = self.feed_fwd(x)\n",
        "    x = x + residual\n",
        "    return x\n",
        "\n",
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "  vocab_size: int = 50304\n",
        "  block_size: int = 1024\n",
        "  n_layer: int = 7\n",
        "  n_head: int = 4\n",
        "  emb_dim: int = 64\n",
        "  bias: bool = True\n",
        "  dropout: float = 0.0\n",
        "  use_torch_mhattention: bool = True\n",
        "\n",
        "  def __init__(self, gpt_cfg):\n",
        "    self.vocab_size = gpt_cfg.vocab_size\n",
        "    self.block_size = gpt_cfg.n_positions\n",
        "    self.n_layer = gpt_cfg.n_layer\n",
        "    self.n_head = gpt_cfg.n_head\n",
        "    self.emb_dim = gpt_cfg.n_embd\n",
        "    self.bias = True\n",
        "    self.droput = 0.0\n",
        "\n",
        "class N00bGPTLMHeadModel(torch.nn.Module):\n",
        "  def __init__(self, cfg: ModelConfig):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.emb_dim = self.cfg.emb_dim\n",
        "\n",
        "    self.token_emb = torch.nn.Embedding(self.cfg.vocab_size, self.emb_dim)\n",
        "    self.pos_emb = torch.nn.Embedding(self.cfg.block_size, self.emb_dim)\n",
        "    self.dropout = torch.nn.Dropout(self.cfg.dropout)\n",
        "    self.layers = torch.nn.ModuleList([Layer(cfg.emb_dim, cfg.n_head, cfg.bias, cfg.dropout) for _ in range(self.cfg.n_layer)])\n",
        "    self.ln = torch.nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "    self.lang_model_head = torch.nn.Linear(self.emb_dim, self.cfg.vocab_size, bias=False) # Do we need to explicitly disable bias here?\n",
        "\n",
        "  def base_forward(self, x):\n",
        "    n_batch, seq_len = x.shape\n",
        "    assert seq_len <= self.cfg.block_size\n",
        "    pos = torch.arange(0, seq_len)\n",
        "    if torch.cuda.is_available():\n",
        "      pos = pos.to(torch.device(\"cuda\"))\n",
        "\n",
        "    token_emb = self.token_emb(x) # [n_batch, seq_len, emb_dim]\n",
        "    pos_emb = self.pos_emb(pos) # [seq_len, emb_dim]\n",
        "    x = self.dropout(token_emb+pos_emb)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    x = self.ln(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    x = self.base_forward(x)\n",
        "\n",
        "    if not targets is None: # Training\n",
        "      logits = self.lang_model_head(x)\n",
        "      loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "    else: # Inference\n",
        "      logits = self.lang_model_head(x[:,[-1],:]) # Only need to compute lm_head for the last token\n",
        "      loss = None\n",
        "    return logits, loss\n",
        "\n",
        "  def sample(self, ids, len):\n",
        "    for _ in range(len):\n",
        "      logits = self.forward(ids)[0][:,-1,:]\n",
        "      _, id = torch.topk(logits, 1)\n",
        "      ids = torch.concat([ids,id],dim=1)\n",
        "    return ids\n",
        "\n",
        "  @classmethod\n",
        "  def from_hf_pretrained_weight(clas):\n",
        "    cfg_hf = transformers.GPT2Config()\n",
        "    m_hf = transformers.GPT2LMHeadModel(cfg_hf).from_pretrained('gpt2')\n",
        "    cfg = ModelConfig(cfg_hf)\n",
        "    m = N00bGPTLMHeadModel(cfg)\n",
        "\n",
        "    def weight_copy(x,y,src,dst,trans):\n",
        "      for a,b in zip(src, dst):\n",
        "        print(f'{a} -> {b}')\n",
        "        need_transpose = any([a.endswith(s) for s in trans])\n",
        "        if need_transpose:\n",
        "          y.state_dict()[b].copy_(x.state_dict()[a].T)\n",
        "        else:\n",
        "          y.state_dict()[b].copy_(x.state_dict()[a])\n",
        "\n",
        "    src = [str(i) for i in m_hf.state_dict().keys()]\n",
        "    dst = [str(i) for i in m.state_dict().keys()]\n",
        "    trans=['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight','mlp.c_proj.weight']\n",
        "    weight_copy(m_hf,m,src,dst,trans)\n",
        "    return m\n",
        "\n"
      ],
      "metadata": {
        "id": "xkoUGmN3kgcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_util('before creating model')\n",
        "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "cfg_hf = transformers.GPT2Config()\n",
        "cfg = ModelConfig(cfg_hf)\n",
        "m = N00bGPTLMHeadModel(cfg)\n",
        "m = torch.compile(m, backend=\"inductor\")\n",
        "m=m.to(dev)\n",
        "m.train()\n",
        "# opt = torch.optim.SGD(m.parameters(), lr=0.001)\n",
        "opt = torch.optim.Adam(m.parameters(), lr=0.001, betas=(0.9,0.999),eps=1e-7)\n",
        "gpu_util('after creating model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkyFhZWkjNy",
        "outputId": "2b36ec1d-1de4-4c50-bad7-7688acf99e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before creating model    : GPU memory (Torch) total: 15.8, reserved: 0.0, allocated: 0.0, free: 15.8, peak: 0.0\n",
            "after creating model     : GPU memory (Torch) total: 15.8, reserved: 0.7, allocated: 0.6, free: 15.2, peak: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accelerator = Accelerator()\n",
        "m,opt = accelerator.prepare(m,opt)"
      ],
      "metadata": {
        "id": "8VjTDxZy0eIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 36\n",
        "seq_len = 128\n",
        "emb_dim = cfg.emb_dim\n",
        "vocab_size = cfg.vocab_size\n",
        "n_layer = cfg.n_layer\n",
        "n_params = (\n",
        "    vocab_size * emb_dim * 2 + # embedding table\n",
        "    n_layer * (emb_dim ** 2 * 9) + # transformers: attention  + feed forward\n",
        "    emb_dim * vocab_size # lm_head\n",
        ")\n",
        "gb = 1024**3\n",
        "print(f'Model parameters: {n_params/gb * 4:.1f}gb')\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64iDju_LlNIA",
        "outputId": "e889232e-2bcf-417e-eae4-5cdb69e9c315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 0.7gb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "t1 = time.time()\n",
        "n_epoch=200\n",
        "logging_interval=10\n",
        "\n",
        "for i in range(n_epoch):\n",
        "  x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(dev)\n",
        "  y = x\n",
        "  _, loss = m(x,y)\n",
        "  accelerator.backward(loss)\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "  if i % logging_interval == 0:\n",
        "    t2=time.time()\n",
        "    tot = logging_interval *  batch_size\n",
        "    qps = tot / (t2-t1)\n",
        "    t1=t2\n",
        "    print(f'at {i} qps: {qps}')\n",
        "    gpu_util(f'itrn {i}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rfmeKHMknyj",
        "outputId": "a4e3422a-566b-4020-906d-4bf09f8f1524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at 0 qps: 12.019725077075487\n",
            "itrn 0                   : GPU memory (Torch) total: 15.8, reserved: 8.3, allocated: 3.3, free: 12.4, peak: 8.2\n",
            "at 10 qps: 126.89126331961685\n",
            "itrn 10                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 20 qps: 115.94149411665518\n",
            "itrn 20                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 30 qps: 115.2031114121582\n",
            "itrn 30                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 40 qps: 115.01193155448857\n",
            "itrn 40                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 50 qps: 113.85529969678791\n",
            "itrn 50                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 60 qps: 114.4812604581375\n",
            "itrn 60                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 70 qps: 115.21710605532712\n",
            "itrn 70                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 3.3, free: 12.4, peak: 10.8\n",
            "at 80 qps: 115.10096216954436\n",
            "itrn 80                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 90 qps: 112.97148676277745\n",
            "itrn 90                  : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 100 qps: 112.86339230281769\n",
            "itrn 100                 : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 110 qps: 115.35901998696323\n",
            "itrn 110                 : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 120 qps: 114.70749255686776\n",
            "itrn 120                 : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 130 qps: 114.47544533197474\n",
            "itrn 130                 : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 4.2, free: 11.6, peak: 10.8\n",
            "at 140 qps: 113.83070018778925\n",
            "itrn 140                 : GPU memory (Torch) total: 15.8, reserved: 11.7, allocated: 3.3, free: 12.4, peak: 10.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyT7Ncgjo3b3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}