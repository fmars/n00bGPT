{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT2LIQF1stN1Iv3MZVEUgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmars/n00bGPT/blob/main/colab/model_parity_debugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Rc34C8e7o5KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "TVmbIiP6o8Jc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_copy(x,y,src,dst,trans):\n",
        "  for a,b in zip(src, dst):\n",
        "    print(f'{a} -> {b}')\n",
        "    need_transpose = any([a.endswith(s) for s in trans])\n",
        "    if need_transpose:\n",
        "      y.state_dict()[b].copy_(x.state_dict()[a].T)\n",
        "    else:\n",
        "      y.state_dict()[b].copy_(x.state_dict()[a])"
      ],
      "metadata": {
        "id": "jFpkbjTCp8-S"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
        "\n",
        "cfg = GPT2Config()\n",
        "m_ori=transformers.AutoModel.from_pretrained('gpt2')\n",
        "m_lmh= GPT2LMHeadModel(cfg).from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "ElyJs8zrp1qy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaxL6EIyxF7E",
        "outputId": "8f438322-3983-42af-b391-71a1e854e609"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.31.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50257\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "  vocab_size: int = 50304\n",
        "  block_size: int = 1024\n",
        "  n_layer: int = 7\n",
        "  n_head: int = 4\n",
        "  emb_dim: int = 64\n",
        "  bias: bool = True\n",
        "  dropout: float = 0.0\n",
        "  use_torch_mhattention: bool = True\n",
        "\n",
        "  def __init__(self, gpt_cfg):\n",
        "    self.vocab_size = gpt_cfg.vocab_size\n",
        "    self.block_size = gpt_cfg.n_positions\n",
        "    self.n_layer = gpt_cfg.n_layer\n",
        "    self.n_head = gpt_cfg.n_head\n",
        "    self.emb_dim = gpt_cfg.n_embd\n",
        "    self.bias = True\n",
        "    self.droput = 0.0\n",
        "m_cfg = ModelConfig(cfg)"
      ],
      "metadata": {
        "id": "XxpmPxAf_kBv"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "seq_len = 64\n",
        "emb_dim = m_lmh.config.n_embd\n",
        "n_head = m_lmh.config.n_head\n",
        "bias = True\n",
        "dropout = 0"
      ],
      "metadata": {
        "id": "rTqJHZ3fCw6V"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Attention <> MultiheadAttention <> torch.nn.MultiheadAttention"
      ],
      "metadata": {
        "id": "-zpUj9whovJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = m_lmh.transformer.h[0].attn\n",
        "print(type(x))"
      ],
      "metadata": {
        "id": "2dTZk4gapKWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e33271-8f64-4b3f-a01c-f06d82a60961"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Attention'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, n_head):\n",
        "    super().__init__()\n",
        "    self.emb_dim = emb_dim\n",
        "    self.n_head = n_head\n",
        "    self.head_dim = emb_dim // n_head\n",
        "    assert self.head_dim * self.n_head == self.emb_dim\n",
        "    self.bias = True\n",
        "\n",
        "    self.attn = torch.nn.Linear(self.emb_dim, 3 * self.emb_dim, bias = self.bias)\n",
        "    self.proj = torch.nn.Linear(self.emb_dim, self.emb_dim, bias=self.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: [n_batch, seq_len, emb_dim], assuming q=k=v=x in MultiheadAttentionSimple implementation\n",
        "    n_batch, seq_len, emb_dim = x.shape\n",
        "    # Vectorized/concated form of QW_q_i, KW_k_i, and VW_v_i\n",
        "    attn = self.attn(x) # [n_batch, seq_len, 3*emb_dim]\n",
        "    q,k,v = attn.split(self.emb_dim, dim=-1)\n",
        "    # Reshape to per-head form\n",
        "    q = q.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    k = k.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    v = v.view(n_batch, seq_len, self.n_head, self.head_dim).transpose(1,2)\n",
        "    # Compute dot product attention, which input is [N,..., seq_len, emb_dim]\n",
        "    y = torch.nn.functional.scaled_dot_product_attention(q,k,v, is_causal=True) # [n_batch, n_head, seq_len, head_dim]\n",
        "    y = y.transpose(1,2).contiguous().view(n_batch, seq_len, emb_dim) # [n_batch, seq_len, emb_dim]\n",
        "    y = self.proj(y)\n",
        "    return y\n",
        "y=MultiheadAttention(emb_dim, n_head)"
      ],
      "metadata": {
        "id": "bRis8N4M0jOJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.nn.MultiheadAttention(emb_dim, n_head,batch_first=True)"
      ],
      "metadata": {
        "id": "1HopXHmgq0Z_"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in x.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in y.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in z.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1X3DMgipgb5",
        "outputId": "a1b5559b-f614-47bb-fb06-20eb9bd57dbb"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c_attn.weight -> torch.Size([768, 2304])\n",
            "c_attn.bias -> torch.Size([2304])\n",
            "c_proj.weight -> torch.Size([768, 768])\n",
            "c_proj.bias -> torch.Size([768])\n",
            "------------------------------\n",
            "attn.weight -> torch.Size([2304, 768])\n",
            "attn.bias -> torch.Size([2304])\n",
            "proj.weight -> torch.Size([768, 768])\n",
            "proj.bias -> torch.Size([768])\n",
            "------------------------------\n",
            "in_proj_weight -> torch.Size([2304, 768])\n",
            "in_proj_bias -> torch.Size([2304])\n",
            "out_proj.weight -> torch.Size([768, 768])\n",
            "out_proj.bias -> torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in y.state_dict().keys()]\n",
        "trans=['c_attn.weight', 'c_proj.weight']\n",
        "weight_copy(x,y,src,dst,trans)\n",
        "\n",
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in z.state_dict().keys()]\n",
        "trans=['c_attn.weight', 'c_proj.weight']\n",
        "weight_copy(x,z,src,dst,trans)"
      ],
      "metadata": {
        "id": "IPBWRPYEpyVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4064c82b-3bdf-4fea-de94-ae2463a8b38a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c_attn.weight -> attn.weight\n",
            "c_attn.bias -> attn.bias\n",
            "c_proj.weight -> proj.weight\n",
            "c_proj.bias -> proj.bias\n",
            "c_attn.weight -> in_proj_weight\n",
            "c_attn.bias -> in_proj_bias\n",
            "c_proj.weight -> out_proj.weight\n",
            "c_proj.bias -> out_proj.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=k=v = torch.randn(batch_size, seq_len, emb_dim)\n",
        "attn_mask = torch.triu(torch.ones(q.size(1),q.size(1)),diagonal=1)\n",
        "attn_mask = attn_mask.bool()\n",
        "o_x = x(q)[0]\n",
        "o_y = y(q)\n",
        "o_z = z(q,k,v, attn_mask=attn_mask,is_causal=True)[0]\n",
        "\n",
        "print(f'x <> y {torch.max(o_x-o_y)} {torch.allclose(o_x, o_y,atol=1e-3)}')\n",
        "print(f'x <> z {torch.max(o_x-o_z)} {torch.allclose(o_x, o_z,atol=1e-3)}')\n",
        "print(f'y <> z {torch.max(o_y-o_z)} {torch.allclose(o_y, o_z,atol=1e-3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D9g_Q0h9ONg",
        "outputId": "272728da-0d57-49b2-9aad-ba641314fbb7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x <> y 0.0002288818359375 True\n",
            "x <> z 0.0002624988555908203 True\n",
            "y <> z 0.00019073486328125 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2MLP <> FeedForward"
      ],
      "metadata": {
        "id": "r0tS3T7uuYW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = m_lmh.transformer.h[0].mlp\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX5HqqvWugR-",
        "outputId": "811a6986-8c0f-429a-e5b3-ac7703c32d6b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2MLP'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.activations import ACT2FN\n",
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, bias, dropout):\n",
        "    super().__init__()\n",
        "    self.context_proj_1 = torch.nn.Linear(emb_dim, 4 *emb_dim, bias=bias)\n",
        "    self.gelu = torch.nn.GELU()\n",
        "    self.gelu = ACT2FN['gelu_new']\n",
        "    self.context_proj_2 = torch.nn.Linear(4*emb_dim, emb_dim, bias=bias)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.context_proj_1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.context_proj_2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "y = FeedForward(emb_dim, bias, dropout)"
      ],
      "metadata": {
        "id": "fNGujXo2uuuf"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in x.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in y.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWYWpMNOvJ7M",
        "outputId": "b42324fc-0d63-49fe-b6a1-ac8fcbb857de"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c_fc.weight -> torch.Size([768, 3072])\n",
            "c_fc.bias -> torch.Size([3072])\n",
            "c_proj.weight -> torch.Size([3072, 768])\n",
            "c_proj.bias -> torch.Size([768])\n",
            "------------------------------\n",
            "context_proj_1.weight -> torch.Size([3072, 768])\n",
            "context_proj_1.bias -> torch.Size([3072])\n",
            "context_proj_2.weight -> torch.Size([768, 3072])\n",
            "context_proj_2.bias -> torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in y.state_dict().keys()]\n",
        "trans=['c_fc.weight', 'c_proj.weight']\n",
        "weight_copy(x,y,src,dst,trans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caGjQfhDvOoF",
        "outputId": "5df01658-0471-4c36-d079-778407a78415"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c_fc.weight -> context_proj_1.weight\n",
            "c_fc.bias -> context_proj_1.bias\n",
            "c_proj.weight -> context_proj_2.weight\n",
            "c_proj.bias -> context_proj_2.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(batch_size, seq_len, emb_dim)\n",
        "o_x = x(input)\n",
        "o_y = y(input)\n",
        "print(f'x <> y {torch.max(o_x-o_y)} {torch.allclose(o_x, o_y)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8TggvP1vWzB",
        "outputId": "47f6a139-4418-4d0c-9bef-9f9d9b75d07b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x <> y 0.0 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Block <> Layer"
      ],
      "metadata": {
        "id": "i8HJIvUwyShZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = m_lmh.transformer.h[0]\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raq0ualoyVc6",
        "outputId": "a4bdf994-20cb-4cbc-89db-9b2c61284668"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Block'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(torch.nn.Module):\n",
        "  def __init__(self, emb_dim, n_head, bias, dropout):\n",
        "    super().__init__()\n",
        "    self.ln_1 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.attn = MultiheadAttention(emb_dim, n_head)\n",
        "    self.ln_2 = torch.nn.LayerNorm(emb_dim)\n",
        "    self.feed_fwd = FeedForward(emb_dim, bias, dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.ln_1(x)\n",
        "    x = self.attn(x)\n",
        "    x = x + residual\n",
        "    residual = x\n",
        "    x = self.ln_2(x)\n",
        "    x = self.feed_fwd(x)\n",
        "    x = x + residual\n",
        "    return x\n",
        "y = Layer(emb_dim, n_head, bias, dropout)"
      ],
      "metadata": {
        "id": "4T25aQ0kyZMq"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in x.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in y.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnMDIhdlysiY",
        "outputId": "3f5e4e9d-a7fc-4ae0-cfd5-542b388819c5"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln_1.weight -> torch.Size([768])\n",
            "ln_1.bias -> torch.Size([768])\n",
            "attn.c_attn.weight -> torch.Size([768, 2304])\n",
            "attn.c_attn.bias -> torch.Size([2304])\n",
            "attn.c_proj.weight -> torch.Size([768, 768])\n",
            "attn.c_proj.bias -> torch.Size([768])\n",
            "ln_2.weight -> torch.Size([768])\n",
            "ln_2.bias -> torch.Size([768])\n",
            "mlp.c_fc.weight -> torch.Size([768, 3072])\n",
            "mlp.c_fc.bias -> torch.Size([3072])\n",
            "mlp.c_proj.weight -> torch.Size([3072, 768])\n",
            "mlp.c_proj.bias -> torch.Size([768])\n",
            "------------------------------\n",
            "ln_1.weight -> torch.Size([768])\n",
            "ln_1.bias -> torch.Size([768])\n",
            "attn.attn.weight -> torch.Size([2304, 768])\n",
            "attn.attn.bias -> torch.Size([2304])\n",
            "attn.proj.weight -> torch.Size([768, 768])\n",
            "attn.proj.bias -> torch.Size([768])\n",
            "ln_2.weight -> torch.Size([768])\n",
            "ln_2.bias -> torch.Size([768])\n",
            "feed_fwd.context_proj_1.weight -> torch.Size([3072, 768])\n",
            "feed_fwd.context_proj_1.bias -> torch.Size([3072])\n",
            "feed_fwd.context_proj_2.weight -> torch.Size([768, 3072])\n",
            "feed_fwd.context_proj_2.bias -> torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in y.state_dict().keys()]\n",
        "trans=['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight','mlp.c_proj.weight']\n",
        "weight_copy(x,y,src,dst,trans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQV5UJUuy2z0",
        "outputId": "42dcbf10-49b2-4396-ebaf-6f686e5e4252"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln_1.weight -> ln_1.weight\n",
            "ln_1.bias -> ln_1.bias\n",
            "attn.c_attn.weight -> attn.attn.weight\n",
            "attn.c_attn.bias -> attn.attn.bias\n",
            "attn.c_proj.weight -> attn.proj.weight\n",
            "attn.c_proj.bias -> attn.proj.bias\n",
            "ln_2.weight -> ln_2.weight\n",
            "ln_2.bias -> ln_2.bias\n",
            "mlp.c_fc.weight -> feed_fwd.context_proj_1.weight\n",
            "mlp.c_fc.bias -> feed_fwd.context_proj_1.bias\n",
            "mlp.c_proj.weight -> feed_fwd.context_proj_2.weight\n",
            "mlp.c_proj.bias -> feed_fwd.context_proj_2.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.rand(batch_size, seq_len, emb_dim)\n",
        "o_x = x(input)[0]\n",
        "o_y = y(input)\n",
        "print(f'x <> y {torch.max(o_x-o_y)} {torch.allclose(o_x, o_y, atol=1e-5)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGpJ7OcBzCfK",
        "outputId": "dd95adb4-1745-480b-8045-c474c7aa3d49"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x <> y 1.52587890625e-05 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2Model <> N00bGPT"
      ],
      "metadata": {
        "id": "9fH3ZnYa-m9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = m_lmh.transformer\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmAPykNY-zdW",
        "outputId": "e6459347-0009-43c0-df8d-58107e6238b9"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLMHeadModel(torch.nn.Module):\n",
        "  def __init__(self, cfg: ModelConfig):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    self.emb_dim = self.cfg.emb_dim\n",
        "\n",
        "    self.token_emb = torch.nn.Embedding(self.cfg.vocab_size, self.emb_dim)\n",
        "    self.pos_emb = torch.nn.Embedding(self.cfg.block_size, self.emb_dim)\n",
        "    self.dropout = torch.nn.Dropout(self.cfg.dropout)\n",
        "    self.layers = torch.nn.ModuleList([Layer(cfg.emb_dim, cfg.n_head, cfg.bias, cfg.dropout) for _ in range(self.cfg.n_layer)])\n",
        "    self.ln = torch.nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "    self.lang_model_head = torch.nn.Linear(self.emb_dim, self.cfg.vocab_size, bias=False) # Do we need to explicitly disable bias here?\n",
        "\n",
        "  def base_forward(self, x):\n",
        "    n_batch, seq_len = x.shape\n",
        "    assert seq_len <= self.cfg.block_size\n",
        "    pos = torch.arange(0, seq_len)\n",
        "    if torch.cuda.is_available():\n",
        "      pos = pos.to(torch.device(\"cuda\"))\n",
        "\n",
        "    token_emb = self.token_emb(x) # [n_batch, seq_len, emb_dim]\n",
        "    pos_emb = self.pos_emb(pos) # [seq_len, emb_dim]\n",
        "    x = self.dropout(token_emb+pos_emb)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    x = self.ln(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    x = self.base_forward(x)\n",
        "\n",
        "    if targets: # Training\n",
        "      logits = self.lm_head(x)\n",
        "      loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "    else: # Inference\n",
        "      logits = self.lang_model_head(x[:,[-1],:]) # Only need to compute lm_head for the last token\n",
        "      loss = None\n",
        "    return logits, loss\n",
        "\n",
        "y = GPTLMHeadModel(m_cfg)"
      ],
      "metadata": {
        "id": "1gUdI1p6-3D6"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in x.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in y.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')"
      ],
      "metadata": {
        "id": "Q-2JP-3CAV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in y.state_dict().keys()]\n",
        "trans=['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight','mlp.c_proj.weight']\n",
        "weight_copy(x,y,src,dst,trans)"
      ],
      "metadata": {
        "id": "igY9lLPjAouR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randint(0, m_cfg.vocab_size, (batch_size, seq_len))\n",
        "o_x = x(input)[0]\n",
        "o_y = y.base_forward(input)\n",
        "print(f'x <> y {torch.max(o_x-o_y)} {torch.allclose(o_x, o_y, atol=1e-4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJYxFxu4BHKz",
        "outputId": "225a6401-ed2c-4594-f85b-be12f8ee9fdc"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x <> y 9.1552734375e-05 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2LMHeadModel <> GPTLMHeadModel"
      ],
      "metadata": {
        "id": "J698-A7HDPn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = m_lmh\n",
        "print(type(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFgalcB5DZCC",
        "outputId": "4ecce2f6-72f8-4f9f-f7bf-3ca0164a4b1b"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = GPTLMHeadModel(m_cfg)"
      ],
      "metadata": {
        "id": "aCmdp-OyDbsO"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in x.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')\n",
        "print('-'*30)\n",
        "for k,v in y.state_dict().items():\n",
        "  print(f'{k} -> {v.shape}')"
      ],
      "metadata": {
        "id": "NWWr6TOdDdA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = [str(i) for i in x.state_dict().keys()]\n",
        "dst = [str(i) for i in y.state_dict().keys()]\n",
        "trans=['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight','mlp.c_proj.weight']\n",
        "weight_copy(x,y,src,dst,trans)"
      ],
      "metadata": {
        "id": "uY2XJI--Dfhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=transformers.AutoTokenizer.from_pretrained('gpt2')\n",
        "s='i am a software engineer and i like to'\n",
        "input_ids=tokenizer(s,return_tensors='pt')['input_ids']"
      ],
      "metadata": {
        "id": "9gkXiPvYDkH1"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = input_ids\n",
        "for i in range(20):\n",
        "  outs=x(ids)[0][:,-1,:]\n",
        "  _, id = torch.topk(outs,1)\n",
        "  ids = torch.concat([ids, id],dim=1)\n",
        "tokenizer.decode(ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "50qfhGbVDn_2",
        "outputId": "49b74090-165a-440f-cf48-68f27b454986"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am a software engineer and i like to write code. I am also a programmer. I am a big fan of the open source community.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = input_ids\n",
        "for i in range(20):\n",
        "  outs=x(ids)[0][:,-1,:]\n",
        "  _, id = torch.topk(outs,1)\n",
        "  ids = torch.concat([ids, id],dim=1)\n",
        "tokenizer.decode(ids[0])"
      ],
      "metadata": {
        "id": "NZ0WUfQiEJbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = input_ids\n",
        "for i in range(20):\n",
        "  outs=y(ids)[0][:,-1,:]\n",
        "  _, id = torch.topk(outs,1)\n",
        "  ids = torch.concat([ids, id],dim=1)\n",
        "tokenizer.decode(ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Co6RR_F7EdiH",
        "outputId": "6b23b471-009f-4f9f-cb0b-10cd8a540ae1"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i am a software engineer and i like to write code. I am also a programmer. I am a big fan of the open source community.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l87oEo5XEt0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}